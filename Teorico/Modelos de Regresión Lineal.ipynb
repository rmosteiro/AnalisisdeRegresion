{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP4LJtgJkTxu/n3yemw26Iz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introducción - Repaso"],"metadata":{"id":"VNR23JcqUphA"}},{"cell_type":"markdown","source":["## Modelo de Regresión Lineal Múltiple\n","\n","El Modelo de Regresión Lineal Múltiple (MRLM) se puede expresar, a través de la formulación **escalar**, a partir de la siguiente ecuación:\n","\n","\\begin{equation}\\boxed{\n","  y_i \\;=\\; \\beta_0 \\;+\\; \\sum_{k=1}^{K} \\beta_k\\,x_{ki} \\;+\\; \\varepsilon_i,\n","  \\qquad  i = 1,\\dots,n }\n","\\end{equation}\n","\n","donde $y_i$ es la variable dependiente (explicada), $x_{k}$ las \\(K\\)  variables independientes (explicativas) y $\\varepsilon_i$ el término de error (residuo) de predicción para cada observación $i$.\n","\n","En términos generales, a pesar de que la formulación escalar es intuitivamente más sencilla de interpretar, por simplicidad en las derivaciones matemáticas, se suele expresar el MRLM de **forma matricial**, como se exhibe en la ecuación debajo:\n","\n","\\begin{equation}\\boxed{\n","  \\mathbf{y} \\;=\\; \\mathbf{X}\\boldsymbol{\\beta} \\;+\\; \\boldsymbol{\\varepsilon},\n","  \\qquad\n","  \\mathbf{y}\\in\\mathbb{R}^{n\\times 1},\\;\n","  \\mathbf{X}\\in\\mathbb{R}^{n\\times (K+1)},\\;\n","  \\boldsymbol{\\beta}\\in\\mathbb{R}^{(K+1)\\times 1}.}\n","\\end{equation}\n","\n","---\n","\n","### Hipótesis Básicas sobre los Residuos\n","\n","1. **Esperanza nula**\n","\n","   $$\n","   \\mathbb{E}({\\boldsymbol\\varepsilon})=0\n","   $$\n","\n","2. **Varianza constante (homocedasticidad)**\n","\n","   $$\n","   \\operatorname{Var}({\\boldsymbol\\varepsilon})=\\sigma^{2}\n","   $$\n","\n","3. **Distribución normal**\n","\n","   $$\n","   {\\boldsymbol\\varepsilon} \\sim \\mathcal{N}\\bigl(0,\\sigma^{2}\\bigr)\n","   $$\n","\n","4. **Independencia entre perturbaciones**\n","\n","   $$\n","   \\mathbb{E}(\\varepsilon_i,\\varepsilon_j)=0,\\quad i\\neq j\n","   $$\n","\n","---\n"],"metadata":{"id":"DszcjyDXU5SQ"}},{"cell_type":"markdown","source":["\n","## Método de Estimación (Máxima Verosimilitud)\n","\n","#### ¿Qué es y para qué sirve la **Máxima Verosimilitud** (MLE)?\n","\n","**Idea básica**  \n","La verosimilitud mide cuán plausible es un conjunto de parámetros dado lo que observamos.  \n","Formalmente, para un vector de parámetros $\\boldsymbol\\theta$ y datos $\\mathbf{y}$:\n","\n","$$\\boxed{\n","L(\\boldsymbol\\theta \\mid \\mathbf{y}) \\;=\\; f_{\\mathbf{Y}}(\\mathbf{y}; \\boldsymbol\\theta)},\n","$$\n","\n","donde $L(\\boldsymbol\\theta \\mid \\mathbf{y})$ es la **función de verosimilitud**, dada por $f_{\\mathbf{Y}}$ que es la función de densidad conjunta del vector aleatorio $Y$ evaluada en la realización $y$ y parametrizada en $\\boldsymbol\\theta$.  \n","\n","El **estimador de Máxima Verosimilitud (MLE)** es\n","\n","$$\n","\\boxed{\\hat{\\boldsymbol\\theta}_{\\text{MLE}}\n","\\;=\\;\n","\\arg\\max_{\\boldsymbol\\theta}\\; L(\\boldsymbol\\theta \\mid \\mathbf{y})}.\n","$$\n","\n","En palabras: «elige el valor de $\\boldsymbol\\theta$ que hace que los datos observados  \n","sean lo más “probables” posible».\n","\n","#### MLE para la **Regresión Lineal Múltiple**\n","\n","Bajo el supuesto de errores normales, el log‑verosímil es\n","\n","$$\\boxed{\n","\\ell(\\boldsymbol\\beta, \\sigma^{2})\n","= -\\frac{n}{2}\\log(2\\pi)\\;\n","  -\\frac{n}{2}\\log(\\sigma^{2})\n","  -\\frac{1}{2\\sigma^{2}}\n","    (\\mathbf{y}-\\mathbf{X}\\boldsymbol\\beta)^{\\!\\top}\n","    (\\mathbf{y}-\\mathbf{X}\\boldsymbol\\beta)\\,}.\n","$$\n","\n","* **Maximizar** $\\ell$ respecto a $\\boldsymbol\\beta$ **equivale**\n","  a **minimizar la suma de cuadrados de los residuos**:\n","  $$\\text{SCR}=(\\mathbf{y}-\\mathbf{X}\\boldsymbol\\beta)^{\\top}(\\mathbf{y}-\\mathbf{X}\\boldsymbol\\beta)$$.  \n","  Por eso el MLE de $\\boldsymbol\\beta$ coincide con el estimador de\n","  **Mínimos Cuadrados Ordinarios (MCO)**.\n","\n","* El MLE de $\\sigma^{2}$ es la **media** del residuo cuadrado\n","  $(\\text{SCR}/n)$; si queremos insesgadez usamos $\\text{SCR}/(n-k)$.\n","\n","En suma, **MCO es MLE** a la luz de errores normales y homocedásticos.\n","\n","\n","---\n"],"metadata":{"id":"VacDAf5W7Bix"}},{"cell_type":"markdown","source":["## Método de Estimación (Mínimos Cuadrados Ordinarios)\n","\n","La suma de cuadrados de los residuos (SCR) se define como\n","\n","\\begin{equation}\n","  \\text{SCR}(\\boldsymbol{\\beta})\n","  \\;=\\;\n","  \\bigl(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\bigr)^{\\!\\top}\n","  \\bigl(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\bigr).\n","\\end{equation}\n","\n","Derivamos respecto de $\\boldsymbol{\\beta}$ e igualamos a cero:\n","\n","\\begin{equation}\n","  \\frac{\\partial\\,\\text{SCR}}{\\partial\\boldsymbol{\\beta}}\n","  \\;=\\;\n","  -2\\,\\mathbf{X}^{\\top}\\bigl(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\bigr)\n","  \\;=\\; \\mathbf{0}.\n","\\end{equation}\n","\n","Esto conduce a las **ecuaciones normales**:\n","\n","\\begin{equation}\n","  \\mathbf{X}^{\\top}\\mathbf{X}\\,\\hat{\\boldsymbol{\\beta}}\n","  \\;=\\;\n","  \\mathbf{X}^{\\top}\\mathbf{y}.\n","\\end{equation}\n","\n","Si $\\mathbf{X}^{\\top}\\mathbf{X}$ es invertible, el estimador de MCO es\n","\\begin{equation}\n","  \\boxed{\\;\n","    \\hat{\\boldsymbol{\\beta}}\n","    \\;=\\;\n","    \\bigl(\\mathbf{X}^{\\top}\\mathbf{X}\\bigr)^{-1}\\mathbf{X}^{\\top}\\mathbf{y}\n","  \\;}\n","\\end{equation}\n","\n","---"],"metadata":{"id":"PJ0wQmCL7L3S"}},{"cell_type":"code","source":["# Cargo Paquetes\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import statsmodels.api as sm\n","import statsmodels.formula.api as smf\n","from statsmodels.tools.eval_measures import rmse\n","\n","# Datos\n","datos = pd.read_csv(\"Advertising.csv\")\n","print(datos.head())\n","\n","### Ejemplo Regresión Lineal Simple\n","mod_simple = smf.ols(\"sales ~ TV\", data=datos).fit()\n","datos[\"y_hat\"] = mod_simple.fittedvalues\n","datos[\"resid\"] = mod_simple.resid\n","\n","## Gráfico\n","plt.figure(figsize=(6,4))\n","plt.scatter(datos[\"TV\"], datos[\"sales\"], label=\"Datos\", alpha=0.7)\n","plt.plot(datos[\"TV\"], datos[\"y_hat\"], color=\"navy\", label=\"Recta OLS\")\n","for _, row in datos.iterrows():\n","    plt.plot([row[\"TV\"], row[\"TV\"]], [row[\"sales\"], row[\"y_hat\"]], color=\"gray\", lw=0.6)\n","plt.xlabel(\"Inversión en TV (k$)\")\n","plt.ylabel(\"Ventas\")\n","plt.title(\"Regresión Lineal Simple\")\n","plt.legend()\n","plt.tight_layout()\n","plt.show()\n","\n","# Ejemplo Regresión Lineal Múltiple (dos variables explicativas)\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","mod_multi = smf.ols(\"sales ~ TV + radio\", data=datos).fit()\n","\n","## Gráfico\n","# Grid\n","tv_grid, radio_grid = np.meshgrid(\n","        np.linspace(datos.TV.min(), datos.TV.max(), 30),\n","        np.linspace(datos.radio.min(), datos.radio.max(), 30)\n",")\n","grid_df = pd.DataFrame({\"TV\": tv_grid.ravel(), \"radio\": radio_grid.ravel()})\n","sales_pred_grid = mod_multi.predict(grid_df).values.reshape(tv_grid.shape)\n","\n","# Visualización\n","fig = plt.figure(figsize=(7,5))\n","ax = fig.add_subplot(111, projection=\"3d\")\n","ax.scatter(datos[\"TV\"], datos[\"radio\"], datos[\"sales\"], c=\"r\", label=\"Datos\")     # puntos\n","ax.plot_surface(tv_grid, radio_grid, sales_pred_grid, alpha=0.4, color=\"steelblue\")  # plano OLS\n","\n","ax.set_xlabel(\"TV (k$)\")\n","ax.set_ylabel(\"Radio (k$)\")\n","ax.set_zlabel(\"Ventas\")\n","ax.set_title(\"Regresión Lineal Múltiple\")\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"aYthgpZmaH0S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Métricas de Bondad de Ajuste"],"metadata":{"id":"k1sY3hsWJlA2"}},{"cell_type":"markdown","source":["## Descomposición de la varianza  \n","\n","En un modelo de regresión lineal se obtienen tres sumas de cuadrados que se relacionan vía la siguiente ecuación:\n","\n","$$\n","\\boxed{\\;\n","\\text{SCT}\n","\\;=\\;\n","\\text{SCE}\n","\\;+\\;\n","\\text{SCR}\n","\\;}\n","$$\n","\n","| Sigla | Fórmula | ¿Qué captura? |\n","|-------|---------|---------------|\n","| **SCT** (Suma de Cuadrados Totales) | $\\scriptsize\\displaystyle\\sum_{i=1}^{n}(y_i-\\bar y)^2$ | Variabilidad total de la variable dependiente. |\n","| **SCE** (Suma de Cuadrados *Explicada*) | $\\scriptsize\\displaystyle\\sum_{i=1}^{n}(\\hat y_i-\\bar y)^2$ | Parte de la variabilidad explicada por el modelo. |\n","| **SCR** (Suma de Cuadrados *Residual*) | $\\scriptsize\\displaystyle\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2$ | Variabilidad que **queda sin explicar** (errores).<br>Es la que minimiza MCO/MLE. |\n","\n","---\n","\n","### Coeficiente de determinación $R^{2}$\n","\n","$$\n","R^{2}\n","\\;=\\;\n","1-\\frac{\\text{SCR}}{\\text{SCT}}\n","\\;=\\;\n","\\frac{\\text{SCE}}{\\text{SCT}}.\n","$$\n","\n","* **Intuición**: fracción de la variabilidad de \\(y\\) explicada por los regresores.  \n","* **Rango**: $0 \\le R^{2} \\le 1$  \n","  * $R^{2}=1$: ajuste perfecto (SCR = 0).  \n","  * $R^{2}=0$: el modelo no mejora la media ($\\hat y_i = \\bar y$).  \n","* **Objetivo**: indicar **qué tan bien** el modelo «encaja» a los datos.\n","* **Advertencia**: 1) **No mide causalidad ni validez externa**,\n","2) Siempre crece al añadir regresores; para penalizar parámetros extra se usa el $R^{2}$ **ajustado**.\n","\n","$$\n","R^{2}_{\\text{ajustado}}\n","\\,=\\;\n","1 \\;-\\; (1 - R^{2})\\,\\frac{n - 1}{\\,n - p - 1\\,},\n","$$\n","\n","donde  \n","\\(n\\) es el número total de observaciones y  \n","\\(p\\) es el número de predictores (sin contar el intercepto).\n","\n","---\n","\n","### Error Cuadrático Medio (ECM) / Raíz del ECM\n","\n","$$\n","\\text{ECM}\n","\\;=\\;\n","\\frac{\\text{SCR}}{n-k}\n",".\n","$$\n","\n","* **Intuición**: promedio del **cuadrado** de los residuos.  \n","* **Unidades**: cuadradas (p. ej. pesos², grados²).  \n","* **Objetivo**: medir la **pérdida** del modelo; útil para comparar dos modelos sobre los mismos datos.  \n","* **Raíz cuadrada** ➜ **RMSE** (mismo orden de magnitud que \\(y\\)).\n","\n","---\n"],"metadata":{"id":"gKZ_ZNvKK7tl"}},{"cell_type":"code","source":["### Modelo de Regresión Lineal\n","mod = smf.ols(\"sales ~ TV + radio + newspaper\", data=datos).fit()\n","print(mod.summary())\n","\n","### Métricas de Bondad de Ajuste\n","# R2 Ajustado\n","print(\"R2 Ajustado: \", mod.rsquared_adj)\n","# ECM\n","print(\"ECM: \", mod.mse_resid)\n","# RMSE\n","print(\"RMSE: \", rmse(mod.fittedvalues, datos.sales))"],"metadata":{"id":"-Nogk_7XRgEi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Separación de la Muestra"],"metadata":{"id":"FlTaBGqIKGR9"}},{"cell_type":"markdown","source":["## Precisión vs Complejidad\n","\n","A pesar de que **un modelo cada vez más complejo suele reducir el error de predicción sobre los datos con los que se entrena**, la capacidad de **generalizar** —es decir, de predecir correctamente en datos nunca vistos— puede deteriorarse cuando el modelo se ajusta “demasiado” a la muestra disponible.  \n","En pocas palabras, **una altísima precisión sobre la información conocida puede traducirse en un esquema lo suficientemente rígido como para no trasladarse a información desconocida**.  \n","Para “vigilar” ese problema separamos los datos en al menos dos partes:\n","\n","<p align=\"center\">\n","  <img src=\"https://raw.githubusercontent.com/rmosteiro/AnalisisdeRegresion/main/Imagenes/TrainTest.png\"\n","       alt=\"Curva sesgo–varianza\"\n","       width=\"480\">\n","  <br>\n","  <em style=\"font-size:85%;\">\n","    Fuente: https://kitchell.github.io/DeepLearningTutorial/1introtodeeplearning.html</i>.\n","  </em>\n","</p>\n","\n","\n","| Conjunto | ¿Para qué sirve? |\n","|----------|------------------|\n","| **Training** | *Estimar* los parámetros (coeficientes, pesos, reglas). |\n","| **Testing**  | Medir la **generalización** real.  |\n","\n","---\n","\n","La relación entre **Precisión y Complejidad** en las muestras de entrenamiento y testeo se puede representar a partir del siguiente gráfico:\n","\n","<p align=\"center\">\n","  <img src=\"https://raw.githubusercontent.com/rmosteiro/AnalisisdeRegresion/main/Imagenes/Bias-Variance.png\"\n","       alt=\"Curva sesgo–varianza\"\n","       width=\"480\">\n","  <br>\n","  <em style=\"font-size:85%;\">\n","    Fuente: Hastie, Tibshirani &amp; Friedman, <i>The Elements of Statistical Learning</i>.\n","  </em>\n","</p>\n","\n","- **Curva celeste (entrenamiento)**: el error siempre baja al añadir complejidad.  \n","- **Curva roja (test)**: primero baja (captura la señal) y luego sube (captura el **ruido**).  \n","- El **valle mínimo** de la curva roja indica la complejidad que mejor **equilibra precisión y complejidad**.\n","- Separar la muestra permite **detectar ese punto óptimo** y quedarnos con el modelo que realmente aprende patrones útiles.\n","\n","---\n","\n","### Subajuste vs Sobreajuste  \n","\n","| Concepto | Síntomas típicos | Diagnóstico (train / test) |\n","|----------|------------------|------------------------------------|\n","| **Subajuste**<br>(_underfitting_) | • Predicciones pobres en **todos** los conjuntos.<br>• Error alto y estable al añadir datos.<br>• Gráficos de residuos muestran patrones claros. | `ECM/MSE_train` **alto** /`ECM/MSE_test` **alto** |\n","| **Sobreajuste**<br>(_overfitting_) | • Error **muy bajo** en train, **alto** en test.<br>• Predicciones inestables ante pequeñas perturbaciones de datos.<br>• Parámetros con varianzas enormes. | `ECM/MSE_train` **bajo** /`ECM/MSE_test` **alto**|\n","\n","#### ¿Cómo se ve en la curva de la sección anterior?\n","\n","* **Subajuste**: parte izquierda de la curva → error alto en train **y** test.  \n","  El modelo no logra capturar la forma real de los datos.  \n","* **Punto óptimo**: valle de la curva roja → mínimo error de generalización.  \n","  Balance entre precisión y complejidad.\n","* **Sobreajuste**: parte derecha → el error en train sigue bajando,  \n","  pero el de test sube; el modelo “memoriza” ruido.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"SS7N1f8pK_qq"}},{"cell_type":"code","source":["# Separación de la Muestra (Train/Test)\n","from sklearn.model_selection import train_test_split\n","\n","train_df, test_df = train_test_split(\n","        datos,\n","        test_size=0.30,\n","        random_state=12345)\n","\n","### Modelo de Regresión Lineal (Train)\n","mod_train = smf.ols(\"sales ~ TV + radio + newspaper\", data=train_df).fit()\n","print(mod_train.summary())\n","\n","## Métricas de Bondad de Ajuste - TRAIN\n","r2_adj_train = mod_train.rsquared_adj\n","ecm_train    = mod_train.mse_resid\n","rmse_train   = rmse(mod_train.fittedvalues, train_df.sales)\n","\n","## Métricas de Bondad de Ajuste - TEST\n","# Observado/Predicho\n","y_test      = test_df[\"sales\"]\n","y_hat_test  = mod.predict(test_df)\n","\n","# R² (no ajustado) en TEST\n","sce = ((y_test - y_hat_test)**2).sum()\n","sct = ((y_test - y_test.mean())**2).sum()\n","r2_test = 1 - sce/sct\n","\n","# R² Ajustado en TEST  (n_test = observaciones de test, p = predictores)\n","sce = ((y_test - y_hat_test)**2).sum()\n","sct = ((y_test - y_test.mean())**2).sum()\n","r2_test = 1 - sce/sct\n","n_test = len(test_df)\n","p = len(mod.params) - 1\n","r2_adj_test = 1 - (1 - r2_test)*(n_test - 1)/(n_test-p-1)\n","\n","# ECM y RMSE en TEST\n","ecm_test  = sce/(n_test)\n","rmse_test = np.sqrt(ecm_test)\n","\n","### Resumen\n","resumen = pd.DataFrame({\n","    \"Conjunto\":      [\"TRAIN\", \"TEST\"],\n","    \"R2 Ajustado\":   [r2_adj_train, r2_adj_test],\n","    \"ECM\":           [ecm_train, ecm_test],\n","    \"RMSE\":          [rmse_train, rmse_test]\n","}).set_index(\"Conjunto\")\n","print(resumen)"],"metadata":{"id":"EHdakD5Vagm0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Descomposición Sesgo - Varianza"],"metadata":{"id":"MYmCOP00KpS4"}},{"cell_type":"markdown","source":["## Derivación\n","En términos generales, un modelo se puede distinguir por su parte Sistemática y Aleatoria, tal que   $Y = f(X) + \\varepsilon$,  donde $\\mathbb{E}[\\varepsilon] = 0$ y   $\\operatorname{Var}(\\varepsilon) = \\sigma_{\\varepsilon}^{2}$. Sea el ajuste de regresión $\\hat f(X)$ en un punto de entrada $X = x_{0}$, la idea clave es separar el **error de predicción esperado** en tres componentes:\n","\n","$$\n","\\operatorname{Err}(x_0)\n","\\;=\\;\n","\\mathbb{E}\\!\\bigl[(Y-\\hat f(x_0))^2 \\,\\big|\\, X=x_0 \\bigr]\n","\\;=\\;\n","\\underbrace{\\sigma_\\varepsilon^{2}}_{\\text{Error irreducible}}\n","\\;+\\;\n","\\underbrace{\\bigl(\\mathbb{E}[\\hat f(x_0)]-f(x_0)\\bigr)^{2}}_{\\text{Sesgo}^2}\n","\\;+\\;\n","\\underbrace{\\operatorname{Var}\\!\\bigl(\\hat f(x_0)\\bigr)}_{\\text{Varianza}}.\n","$$\n","\n","* **Error irreducible** $(\\sigma_\\varepsilon^{2})$  \n","  La varianza del término aleatorio $\\varepsilon$; no depende del modelo.\n","* **Sesgo**  \n","  Distancia al cuadrado entre la predicción promedio $\\mathbb{E}[\\hat f(x_0)]$ y lo observado $f(x_0)$.\n","* **Varianza**  \n","  Variabilidad de $\\hat f(x_0)$."],"metadata":{"id":"e-i9Bv8HKrvl"}},{"cell_type":"markdown","source":["# Regularización en regresión lineal"],"metadata":{"id":"rMaRFzgtbULe"}},{"cell_type":"markdown","source":["## Ridge (‐ penalización **L2**)\n","\n","La idea es **minimizar** la suma de cuadrados residual añadiendo un término que\n","penaliza el tamaño de los coeficientes:\n","\n","$$\n","\\hat{\\boldsymbol\\beta}^{\\text{ridge}}\n","\\;=\\;\n","\\arg\\min_{\\boldsymbol\\beta}\n","\\Biggl\\{\n","\\sum_{i=1}^{N}\n","\\Bigl(y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij}\\beta_j\\Bigr)^{2}\n","\\;+\\;\n","\\lambda\\sum_{j=1}^{p}\\beta_j^{2}\n","\\Biggr\\}.\n","\\tag{1}\n","$$\n","\n","En forma matricial (omitiendo \\(\\beta_0\\) para claridad):\n","\n","$$\n","\\operatorname{RSS}(\\lambda)\n","=\n","(\\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta)^{\\!\\top}\n","(\\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta)\n","\\;+\\;\n","\\lambda\\,\\boldsymbol\\beta^{\\top}\\boldsymbol\\beta.\n","\\tag{2}\n","$$\n","\n","El problema tiene solución analítica:\n","\n","$$\n","\\boxed{\\;\n","\\hat{\\boldsymbol\\beta}^{\\text{ridge}}\n","=\n","\\bigl(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda \\mathbf{I}\\bigr)^{-1}\n","\\mathbf{X}^{\\top}\\mathbf{y}\n","\\;}\n","\\tag{3}\n","$$\n","\n","*Efecto:* todos los coeficientes se “encogen” hacia 0, lo que **aumenta\n","ligeramente el sesgo** pero **reduce con fuerza la varianza**.\n","\n","---\n","\n","## Lasso (‐ penalización **L1**)\n","\n","Reemplazamos la penalización cuadrática por la suma de valores absolutos:\n","\n","$$\n","\\hat{\\boldsymbol\\beta}^{\\text{lasso}}\n","\\;=\\;\n","\\arg\\min_{\\boldsymbol\\beta}\n","\\Biggl\\{\n","\\frac{1}{2}\\sum_{i=1}^{N}\n","\\Bigl(y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij}\\beta_j\\Bigr)^{2}\n","\\;+\\;\n","\\lambda\\sum_{j=1}^{p}\\lvert\\beta_j\\rvert\n","\\Biggr\\}.\n","\\tag{4}\n","$$\n","\n","A diferencia de Ridge, el problema de Lasso *no admite una solución analítica cerrada** cuando las columnas de $\\mathbf{X}$ no son ortogonales. En la práctica se resuelve numéricamente con algoritmos numéricos (LARS, ADMM, entre otros).\n","\n","*Efecto:* además de encoger, puede dejar muchos\n","\\(\\beta_j = 0\\) exactamente, logrando **selección automática de variables**.\n","Varianza ↓↓; sesgo ↑ (solo en los coeficientes forzados a cero).\n","\n","---\n","\n","## Impacto en la descomposición Sesgo–Varianza\n","\n","| Método   | Sesgo | Varianza | Escenario ideal |\n","|----------|-------|----------|-----------------|\n","| Ridge    | ↑ moderado | ↓ significativo | Colinealidad, muchos predictores moderados |\n","| Lasso    | ↑ en coef. eliminados | ↓↓ (menos parámetros) | $p \\gg n$, muchas variables irrelevantes, interpretabilidad |\n","\n","La **elección de $\\lambda$** se realiza con validación cruzada,\n","buscando el punto donde la **reducción de varianza** compensa el\n","**sesgo añadido**, minimizando así el error de generalización.\n","\n"],"metadata":{"id":"J61mQkXBbCxR"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import statsmodels.api as sm\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import r2_score, mean_squared_error\n","\n","## Cargar Datos\n","df = pd.read_csv(\"TelcoCustomer.csv\", sep = \";\")\n","df.info()\n","\n","## Formula a Estimar\n","df_mod = df.drop(columns=\"customerID\")\n","explicada = \"MonthlyCharges\"\n","explicativas = df_mod.columns.drop(explicada)\n","formula = explicada + \" ~ \" + \" + \".join(explicativas)\n","\n","## Generar Matrices Explicadas/Explicativas\n","from patsy import dmatrices\n","y, X = dmatrices(formula, data=df_mod, return_type=\"dataframe\")\n","\n","## Separacion de la Muestra (10% TRAIN, 90% TEST, con efectos puramente pedagógicos para evaluar efecto RIDGE/LASSO)\n","X_tr, X_te, y_tr, y_te = train_test_split(X, y,\n","                                          test_size=0.9,\n","                                          random_state=12345)\n","\n","## Regresion Lineal Multiple\n","modelo_regresion   = sm.OLS(y_tr, X_tr).fit()\n","print(modelo_regresion.summary())\n","\n","## Métricas de Bondad de Ajuste - TRAIN\n","r2_regresion_train = modelo_regresion.rsquared\n","ecm_regresion_train    = modelo_regresion.mse_resid\n","rmse_regresion_train   = np.sqrt(ecm_regresion_train)\n","\n","## Métricas de Bondad de Ajuste - TEST\n","# Predicciones\n","y_hat_te = modelo_regresion.predict(X_te)\n","# R²\n","r2_regresion_test = r2_score(y_te.values, y_hat_te)\n","\n","# ECM (MSE) y RMSE\n","ecm_regresion_test = mean_squared_error(y_te.values, y_hat_te)\n","rmse_regresion_test = np.sqrt(ecm_regresion_test)\n","\n","# Resumen en tabla\n","resumen_regresion = pd.DataFrame({\n","    \"Conjunto\": [\"TRAIN\", \"TEST\"],\n","    \"R2\":       [r2_regresion_train,  r2_regresion_test],\n","    \"ECM\":      [ecm_regresion_train, ecm_regresion_test],\n","    \"RMSE\":     [rmse_regresion_train,rmse_regresion_test]\n","}).set_index(\"Conjunto\")\n","\n","print(resumen_regresion)\n","\n","## Modelos Ridge/ Lasso con alpha = 0.75\n","alpha = 0.75\n","ridge = sm.OLS(y_tr, X_tr).fit_regularized(alpha=alpha, L1_wt=0)\n","lasso = sm.OLS(y_tr, X_tr).fit_regularized(alpha=alpha, L1_wt=1)\n","\n","# Comparacion de Coeficientes\n","coef_ols   = pd.Series(modelo_regresion.params,   index=X_tr.columns, name=\"MCO\")\n","coef_ridge = pd.Series(ridge.params, index=X_tr.columns, name=\"Ridge α=0.75\")\n","coef_lasso = pd.Series(lasso.params, index=X_tr.columns, name=\"Lasso α=0.75\")\n","\n","coef_tab = pd.concat([coef_ols, coef_ridge, coef_lasso], axis=1).round(4)\n","print(coef_tab)\n","# Comparacion Metricas de Ajuste\n","def metricas(name, model, Xtr, Xte, ytr, yte):\n","    yhat_tr = model.predict(Xtr)\n","    yhat_te = model.predict(Xte)\n","    r2_tr   = r2_score(ytr, yhat_tr)\n","    r2_te   = r2_score(yte, yhat_te)\n","    mse_tr  = mean_squared_error(ytr, yhat_tr)\n","    mse_te  = mean_squared_error(yte, yhat_te)\n","    rmse_tr = np.sqrt(mse_tr)\n","    rmse_te = np.sqrt(mse_te)\n","    return pd.Series({\n","        \"R2_train\":   r2_tr,  \"R2_test\":   r2_te,\n","        \"ECM_train\":  mse_tr, \"ECM_test\":  mse_te,\n","        \"RMSE_train\": rmse_tr,\"RMSE_test\": rmse_te\n","    }, name=name)\n","\n","# Construir la tabla\n","tabla_metricas = pd.concat([\n","    metricas(\"MCO\",   modelo_regresion,   X_tr, X_te, y_tr.values, y_te.values),\n","    metricas(\"Ridge\", ridge, X_tr, X_te, y_tr.values, y_te.values),\n","    metricas(\"Lasso\", lasso, X_tr, X_te, y_tr.values, y_te.values)\n","], axis=1).T.round(3)\n","print(tabla_metricas)"],"metadata":{"id":"8RHqfEKLwkjN","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Validación Cruzada (CV: Cross-Validation)"],"metadata":{"id":"OJZqcDUXnv5y"}},{"cell_type":"markdown","source":["## Selección óptima de $\\lambda$ con **K‑fold Cross‑Validation**\n","\n","Para escoger el $\\lambda$ cuyo *sesgo añadido* queda\n","compensado por la *reducción de varianza* se utiliza **validación cruzada**. En particular, en este curso utilizaremos la técnica de validación K-fold.\n","\n","#### ¿Cómo funciona K‑fold CV?\n","\n","1. **Partición**  \n","   Divide los \\(N\\) datos en \\(K\\) bloques (folds) de tamaño idéntico.\n","\n","   <p align=\"center\">\n","  <img src=\"https://raw.githubusercontent.com/rmosteiro/AnalisisdeRegresion/main/Imagenes/Cross-Validation.png\"\n","       alt=\"Curva sesgo–varianza\"\n","       width=\"480\">\n","  <br>\n","  <em style=\"font-size:85%;\">\n","    Fuente: Hastie, Tibshirani &amp; Friedman, <i>The Elements of Statistical Learning</i>.\n","  </em>\n","</p>\n","\n","2. **Bucle en $\\lambda$**  \n","   Para cada $\\lambda$ candidato y para cada fold $k$:  \n","   * Ajusta Ridge/Lasso con $\\lambda$ usando los $K-1$ folds restantes.  \n","   * Mide la pérdida $Loss\\bigl(y_i,\\hat f^{\\;-\\!k}_\\lambda(x_i)\\bigr)$\n","     en las observaciones del bloque restante.\n","\n","3. **Promedio de errores**  \n","\n","   $$\n","   \\operatorname{CV}(\\lambda)\n","   =\n","   \\frac{1}{N}\n","   \\sum_{i=1}^{N} Loss\\bigl(y_i,\\hat f^{\\;-\\kappa(i)}_\\lambda(x_i)\\bigr).\n","   $$\n","4. **Elección del hiper‑parámetro**  \n","\n","   $$\n","   \\hat\\lambda\n","   =\n","   \\arg\\min_\\lambda \\operatorname{CV}(\\lambda).\n","   $$\n","\n","   Ese $\\hat\\lambda$ se halla justo donde la caída de varianza (por la\n","   penalización) es lo suficientemente grande como para compensar el sesgo introducido, logrando el menor error de test posible.\n","5. **Modelo final**  \n","   Entrena Ridge/Lasso con $\\hat\\lambda$ usando **todo** el conjunto de datos.\n","\n","---\n","\n","#### ¿Qué pasa si tomamos \\(K = N\\)? — *Leave‑One‑Out CV* (LOOCV)\n","\n","* Cada fold contiene **una sola** observación.  \n","* Para cada \\(i\\), entrenamos el modelo con las \\(N-1\\) restantes y\n","  predecimos $y_i$.  \n","* Ventajas:  \n","  * **Muy bajo sesgo**: cada entrenamiento usa casi todo el conjunto.  \n","* Desventajas:  \n","  * **Varianza alta**: los modelos entrenados se parecen mucho entre sí, pero\n","    cada error de test se computa con solo **una** observación → estimaciones\n","    ruidosas del error medio.  \n","  * Coste computacional\n","\n","---\n","\n","Mediante este proceso, la **validación cruzada** convierte la idea\n","teórica del equilibrio Sesgo‑Varianza en una **regla empírica** para\n","escoger $\\lambda$, garantizando que Ridge o Lasso aporten la mayor\n","ganancia posible en capacidad de generalización."],"metadata":{"id":"hxMglLUUnqwQ"}},{"cell_type":"code","source":["from sklearn.linear_model import RidgeCV, LassoCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","import numpy as np, pandas as pd\n","from sklearn.metrics import r2_score, mean_squared_error\n","\n","## Validacion Cruzada\n","# Grilla de lambdas\n","alphas = np.logspace(-3, 3, 20)\n","\n","# Ridge y Lasso con CV (sklearn)\n","ridge_cv = make_pipeline(StandardScaler(with_mean=False),\n","                         RidgeCV(alphas=alphas, cv=5))\n","\n","lasso_cv = make_pipeline(StandardScaler(with_mean=False),\n","                         LassoCV(alphas=alphas, cv=5, max_iter=20000))\n","\n","ridge_cv.fit(X_tr, y_tr.values.ravel())\n","lasso_cv.fit(X_tr, y_tr.values.ravel())\n","\n","print(\"α* Ridge :\", ridge_cv[-1].alpha_)\n","print(\"α* Lasso :\", lasso_cv[-1].alpha_)\n","\n","## Métricas\n","def metricas(name, model, Xtr, Xte, ytr, yte):\n","    yhat_tr = model.predict(Xtr)\n","    yhat_te = model.predict(Xte)\n","    r2_tr   = r2_score(ytr, yhat_tr)\n","    r2_te   = r2_score(yte, yhat_te)\n","    mse_tr  = mean_squared_error(ytr, yhat_tr)\n","    mse_te  = mean_squared_error(yte, yhat_te)\n","    rmse_tr = np.sqrt(mse_tr)\n","    rmse_te = np.sqrt(mse_te)\n","    return pd.Series({\n","        \"R2_train\":   r2_tr,  \"R2_test\":   r2_te,\n","        \"ECM_train\":  mse_tr, \"ECM_test\":  mse_te,\n","        \"RMSE_train\": rmse_tr,\"RMSE_test\": rmse_te\n","    }, name=name)\n","\n","# Construir la tabla\n","tabla_metricas = pd.concat([\n","    metricas(\"MCO\",   modelo_regresion,   X_tr, X_te, y_tr.values, y_te.values),\n","    metricas(\"Ridge\", ridge_cv, X_tr, X_te, y_tr.values, y_te.values),\n","    metricas(\"Lasso\", lasso_cv, X_tr, X_te, y_tr.values, y_te.values)\n","], axis=1).T.round(3)\n","print(tabla_metricas)\n","\n"],"metadata":{"id":"PlLa0i3csoJc"},"execution_count":null,"outputs":[]}]}