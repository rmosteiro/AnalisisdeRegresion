{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción - Repaso"
      ],
      "metadata": {
        "id": "VNR23JcqUphA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo de Regresión Lineal Múltiple\n",
        "\n",
        "El Modelo de Regresión Lineal Múltiple (MRLM) se puede expresar, a través de la formulación **escalar**, a partir de la siguiente ecuación:\n",
        "\n",
        "\\begin{equation}\\boxed{\n",
        "  y_i \\;=\\; \\beta_0 \\;+\\; \\sum_{k=1}^{K} \\beta_k\\,x_{ki} \\;+\\; \\varepsilon_i,\n",
        "  \\qquad  i = 1,\\dots,n }\n",
        "\\end{equation}\n",
        "\n",
        "donde $y_i$ es la variable dependiente (explicada), $x_{k}$ las \\(K\\)  variables independientes (explicativas) y $\\varepsilon_i$ el término de error (residuo) de predicción para cada observación $i$.\n",
        "\n",
        "En términos generales, a pesar de que la formulación escalar es intuitivamente más sencilla de interpretar, por simplicidad en las derivaciones matemáticas, se suele expresar el MRLM de **forma matricial**, como se exhibe en la ecuación debajo:\n",
        "\n",
        "\\begin{equation}\\boxed{\n",
        "  \\mathbf{y} \\;=\\; \\mathbf{X}\\boldsymbol{\\beta} \\;+\\; \\boldsymbol{\\varepsilon},\n",
        "  \\qquad\n",
        "  \\mathbf{y}\\in\\mathbb{R}^{n\\times 1},\\;\n",
        "  \\mathbf{X}\\in\\mathbb{R}^{n\\times (K+1)},\\;\n",
        "  \\boldsymbol{\\beta}\\in\\mathbb{R}^{(K+1)\\times 1}.}\n",
        "\\end{equation}\n",
        "\n",
        "---\n",
        "\n",
        "### Hipótesis Básicas sobre los Residuos\n",
        "\n",
        "1. **Esperanza nula**\n",
        "\n",
        "   $$\n",
        "   \\mathbb{E}({\\boldsymbol\\varepsilon})=0\n",
        "   $$\n",
        "\n",
        "2. **Varianza constante (homocedasticidad)**\n",
        "\n",
        "   $$\n",
        "   \\operatorname{Var}({\\boldsymbol\\varepsilon})=\\sigma^{2}\n",
        "   $$\n",
        "\n",
        "3. **Distribución normal**\n",
        "\n",
        "   $$\n",
        "   {\\boldsymbol\\varepsilon} \\sim \\mathcal{N}\\bigl(0,\\sigma^{2}\\bigr)\n",
        "   $$\n",
        "\n",
        "4. **Independencia entre perturbaciones**\n",
        "\n",
        "   $$\n",
        "   \\mathbb{E}(\\varepsilon_i,\\varepsilon_j)=0,\\quad i\\neq j\n",
        "   $$\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "DszcjyDXU5SQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Método de Estimación (Máxima Verosimilitud)\n",
        "\n",
        "#### ¿Qué es y para qué sirve la **Máxima Verosimilitud** (MLE)?\n",
        "\n",
        "**Idea básica**  \n",
        "La verosimilitud mide cuán plausible es un conjunto de parámetros dado lo que observamos.  \n",
        "Formalmente, para un vector de parámetros $\\boldsymbol\\theta$ y datos $\\mathbf{y}$:\n",
        "\n",
        "$$\\boxed{\n",
        "L(\\boldsymbol\\theta \\mid \\mathbf{y}) \\;=\\; f_{\\mathbf{Y}}(\\mathbf{y}; \\boldsymbol\\theta)},\n",
        "$$\n",
        "\n",
        "donde $L(\\boldsymbol\\theta \\mid \\mathbf{y})$ es la **función de verosimilitud**, dada por $f_{\\mathbf{Y}}$ que es la función de densidad conjunta del vector aleatorio $Y$ evaluada en la realización $y$ y parametrizada en $\\boldsymbol\\theta$.  \n",
        "\n",
        "El **estimador de Máxima Verosimilitud (MLE)** es\n",
        "\n",
        "$$\n",
        "\\boxed{\\hat{\\boldsymbol\\theta}_{\\text{MLE}}\n",
        "\\;=\\;\n",
        "\\arg\\max_{\\boldsymbol\\theta}\\; L(\\boldsymbol\\theta \\mid \\mathbf{y})}.\n",
        "$$\n",
        "\n",
        "En palabras: «elige el valor de $\\boldsymbol\\theta$ que hace que los datos observados  \n",
        "sean lo más “probables” posible».\n",
        "\n",
        "#### MLE para la **Regresión Lineal Múltiple**\n",
        "\n",
        "Bajo el supuesto de errores normales, el log‑verosímil es\n",
        "\n",
        "$$\\boxed{\n",
        "\\ell(\\boldsymbol\\beta, \\sigma^{2})\n",
        "= -\\frac{n}{2}\\log(2\\pi)\\;\n",
        "  -\\frac{n}{2}\\log(\\sigma^{2})\n",
        "  -\\frac{1}{2\\sigma^{2}}\n",
        "    (\\mathbf{y}-\\mathbf{X}\\boldsymbol\\beta)^{\\!\\top}\n",
        "    (\\mathbf{y}-\\mathbf{X}\\boldsymbol\\beta)\\,}.\n",
        "$$\n",
        "\n",
        "* **Maximizar** $\\ell$ respecto a $\\boldsymbol\\beta$ **equivale**\n",
        "  a **minimizar la suma de cuadrados de los residuos**:\n",
        "  $$\\text{SCR}=(\\mathbf{y}-\\mathbf{X}\\boldsymbol\\beta)^{\\top}(\\mathbf{y}-\\mathbf{X}\\boldsymbol\\beta)$$.  \n",
        "  Por eso el MLE de $\\boldsymbol\\beta$ coincide con el estimador de\n",
        "  **Mínimos Cuadrados Ordinarios (MCO)**.\n",
        "\n",
        "* El MLE de $\\sigma^{2}$ es la **media** del residuo cuadrado\n",
        "  $(\\text{SCR}/n)$; si queremos insesgadez usamos $\\text{SCR}/(n-k)$.\n",
        "\n",
        "En suma, **MCO es MLE** a la luz de errores normales y homocedásticos.\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "VacDAf5W7Bix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método de Estimación (Mínimos Cuadrados Ordinarios)\n",
        "\n",
        "La suma de cuadrados de los residuos (SCR) se define como\n",
        "\n",
        "\\begin{equation}\n",
        "  \\text{SCR}(\\boldsymbol{\\beta})\n",
        "  \\;=\\;\n",
        "  \\bigl(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\bigr)^{\\!\\top}\n",
        "  \\bigl(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\bigr).\n",
        "\\end{equation}\n",
        "\n",
        "Derivamos respecto de $\\boldsymbol{\\beta}$ e igualamos a cero:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\frac{\\partial\\,\\text{SCR}}{\\partial\\boldsymbol{\\beta}}\n",
        "  \\;=\\;\n",
        "  -2\\,\\mathbf{X}^{\\top}\\bigl(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\bigr)\n",
        "  \\;=\\; \\mathbf{0}.\n",
        "\\end{equation}\n",
        "\n",
        "Esto conduce a las **ecuaciones normales**:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\mathbf{X}^{\\top}\\mathbf{X}\\,\\hat{\\boldsymbol{\\beta}}\n",
        "  \\;=\\;\n",
        "  \\mathbf{X}^{\\top}\\mathbf{y}.\n",
        "\\end{equation}\n",
        "\n",
        "Si $\\mathbf{X}^{\\top}\\mathbf{X}$ es invertible, el estimador de MCO es\n",
        "\\begin{equation}\n",
        "  \\boxed{\\;\n",
        "    \\hat{\\boldsymbol{\\beta}}\n",
        "    \\;=\\;\n",
        "    \\bigl(\\mathbf{X}^{\\top}\\mathbf{X}\\bigr)^{-1}\\mathbf{X}^{\\top}\\mathbf{y}\n",
        "  \\;}\n",
        "\\end{equation}\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "PJ0wQmCL7L3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargo Paquetes\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.tools.eval_measures import rmse\n",
        "\n",
        "# Datos\n",
        "datos = pd.read_csv(\"Advertising.csv\")\n",
        "print(datos.head())\n",
        "\n",
        "### Ejemplo Regresión Lineal Simple\n",
        "mod_simple = smf.ols(\"sales ~ TV\", data=datos).fit()\n",
        "datos[\"y_hat\"] = mod_simple.fittedvalues\n",
        "datos[\"resid\"] = mod_simple.resid\n",
        "\n",
        "## Gráfico\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(datos[\"TV\"], datos[\"sales\"], label=\"Datos\", alpha=0.7)\n",
        "plt.plot(datos[\"TV\"], datos[\"y_hat\"], color=\"navy\", label=\"Recta OLS\")\n",
        "for _, row in datos.iterrows():\n",
        "    plt.plot([row[\"TV\"], row[\"TV\"]], [row[\"sales\"], row[\"y_hat\"]], color=\"gray\", lw=0.6)\n",
        "plt.xlabel(\"Inversión en TV (k$)\")\n",
        "plt.ylabel(\"Ventas\")\n",
        "plt.title(\"Regresión Lineal Simple\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Ejemplo Regresión Lineal Múltiple (dos variables explicativas)\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "mod_multi = smf.ols(\"sales ~ TV + radio\", data=datos).fit()\n",
        "\n",
        "## Gráfico\n",
        "# Grid\n",
        "tv_grid, radio_grid = np.meshgrid(\n",
        "        np.linspace(datos.TV.min(), datos.TV.max(), 30),\n",
        "        np.linspace(datos.radio.min(), datos.radio.max(), 30)\n",
        ")\n",
        "grid_df = pd.DataFrame({\"TV\": tv_grid.ravel(), \"radio\": radio_grid.ravel()})\n",
        "sales_pred_grid = mod_multi.predict(grid_df).values.reshape(tv_grid.shape)\n",
        "\n",
        "# Visualización\n",
        "fig = plt.figure(figsize=(7,5))\n",
        "ax = fig.add_subplot(111, projection=\"3d\")\n",
        "ax.scatter(datos[\"TV\"], datos[\"radio\"], datos[\"sales\"], c=\"r\", label=\"Datos\")     # puntos\n",
        "ax.plot_surface(tv_grid, radio_grid, sales_pred_grid, alpha=0.4, color=\"steelblue\")  # plano OLS\n",
        "\n",
        "ax.set_xlabel(\"TV (k$)\")\n",
        "ax.set_ylabel(\"Radio (k$)\")\n",
        "ax.set_zlabel(\"Ventas\")\n",
        "ax.set_title(\"Regresión Lineal Múltiple\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aYthgpZmaH0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Métricas de Bondad de Ajuste"
      ],
      "metadata": {
        "id": "k1sY3hsWJlA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descomposición de la varianza  \n",
        "\n",
        "En un modelo de regresión lineal se obtienen tres sumas de cuadrados que se relacionan vía la siguiente ecuación:\n",
        "\n",
        "$$\n",
        "\\boxed{\\;\n",
        "\\text{SCT}\n",
        "\\;=\\;\n",
        "\\text{SCE}\n",
        "\\;+\\;\n",
        "\\text{SCR}\n",
        "\\;}\n",
        "$$\n",
        "\n",
        "| Sigla | Fórmula | ¿Qué captura? |\n",
        "|-------|---------|---------------|\n",
        "| **SCT** (Suma de Cuadrados Totales) | $\\scriptsize\\displaystyle\\sum_{i=1}^{n}(y_i-\\bar y)^2$ | Variabilidad total de la variable dependiente. |\n",
        "| **SCE** (Suma de Cuadrados *Explicada*) | $\\scriptsize\\displaystyle\\sum_{i=1}^{n}(\\hat y_i-\\bar y)^2$ | Parte de la variabilidad explicada por el modelo. |\n",
        "| **SCR** (Suma de Cuadrados *Residual*) | $\\scriptsize\\displaystyle\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2$ | Variabilidad que **queda sin explicar** (errores).<br>Es la que minimiza MCO/MLE. |\n",
        "\n",
        "---\n",
        "\n",
        "### Coeficiente de determinación $R^{2}$\n",
        "\n",
        "$$\n",
        "R^{2}\n",
        "\\;=\\;\n",
        "1-\\frac{\\text{SCR}}{\\text{SCT}}\n",
        "\\;=\\;\n",
        "\\frac{\\text{SCE}}{\\text{SCT}}.\n",
        "$$\n",
        "\n",
        "* **Intuición**: fracción de la variabilidad de \\(y\\) explicada por los regresores.  \n",
        "* **Rango**: $0 \\le R^{2} \\le 1$  \n",
        "  * $R^{2}=1$: ajuste perfecto (SCR = 0).  \n",
        "  * $R^{2}=0$: el modelo no mejora la media ($\\hat y_i = \\bar y$).  \n",
        "* **Objetivo**: indicar **qué tan bien** el modelo «encaja» a los datos.\n",
        "* **Advertencia**: 1) **No mide causalidad ni validez externa**,\n",
        "2) Siempre crece al añadir regresores; para penalizar parámetros extra se usa el $R^{2}$ **ajustado**.\n",
        "\n",
        "$$\n",
        "R^{2}_{\\text{ajustado}}\n",
        "\\,=\\;\n",
        "1 \\;-\\; (1 - R^{2})\\,\\frac{n - 1}{\\,n - p - 1\\,},\n",
        "$$\n",
        "\n",
        "donde  \n",
        "\\(n\\) es el número total de observaciones y  \n",
        "\\(p\\) es el número de predictores (sin contar el intercepto).\n",
        "\n",
        "---\n",
        "\n",
        "### Error Cuadrático Medio (ECM) / Raíz del ECM\n",
        "\n",
        "$$\n",
        "\\text{ECM}\n",
        "\\;=\\;\n",
        "\\frac{\\text{SCR}}{n-k}\n",
        ".\n",
        "$$\n",
        "\n",
        "* **Intuición**: promedio del **cuadrado** de los residuos.  \n",
        "* **Unidades**: cuadradas (p. ej. pesos², grados²).  \n",
        "* **Objetivo**: medir la **pérdida** del modelo; útil para comparar dos modelos sobre los mismos datos.  \n",
        "* **Raíz cuadrada** ➜ **RMSE** (mismo orden de magnitud que \\(y\\)).\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "gKZ_ZNvKK7tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Modelo de Regresión Lineal\n",
        "mod = smf.ols(\"sales ~ TV + radio + newspaper\", data=datos).fit()\n",
        "print(mod.summary())\n",
        "\n",
        "### Métricas de Bondad de Ajuste\n",
        "# R2 Ajustado\n",
        "print(\"R2 Ajustado: \", mod.rsquared_adj)\n",
        "# ECM\n",
        "print(\"ECM: \", mod.mse_resid)\n",
        "# RMSE\n",
        "print(\"RMSE: \", rmse(mod.fittedvalues, datos.sales))"
      ],
      "metadata": {
        "id": "-Nogk_7XRgEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separación de la Muestra"
      ],
      "metadata": {
        "id": "FlTaBGqIKGR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precisión vs Complejidad\n",
        "\n",
        "A pesar de que **un modelo cada vez más complejo suele reducir el error de predicción sobre los datos con los que se entrena**, la capacidad de **generalizar** —es decir, de predecir correctamente en datos nunca vistos— puede deteriorarse cuando el modelo se ajusta “demasiado” a la muestra disponible.  \n",
        "En pocas palabras, **una altísima precisión sobre la información conocida puede traducirse en un esquema lo suficientemente rígido como para no trasladarse a información desconocida**.  \n",
        "Para “vigilar” ese problema separamos los datos en al menos dos partes:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/rmosteiro/AnalisisdeRegresion/main/Imagenes/TrainTest.png\"\n",
        "       alt=\"Curva sesgo–varianza\"\n",
        "       width=\"480\">\n",
        "  <br>\n",
        "  <em style=\"font-size:85%;\">\n",
        "    Fuente: https://kitchell.github.io/DeepLearningTutorial/1introtodeeplearning.html</i>.\n",
        "  </em>\n",
        "</p>\n",
        "\n",
        "\n",
        "| Conjunto | ¿Para qué sirve? |\n",
        "|----------|------------------|\n",
        "| **Training** | *Estimar* los parámetros (coeficientes, pesos, reglas). |\n",
        "| **Testing**  | Medir la **generalización** real.  |\n",
        "\n",
        "---\n",
        "\n",
        "La relación entre **Precisión y Complejidad** en las muestras de entrenamiento y testeo se puede representar a partir del siguiente gráfico:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/rmosteiro/AnalisisdeRegresion/main/Imagenes/Bias-Variance.png\"\n",
        "       alt=\"Curva sesgo–varianza\"\n",
        "       width=\"480\">\n",
        "  <br>\n",
        "  <em style=\"font-size:85%;\">\n",
        "    Fuente: Hastie, Tibshirani &amp; Friedman, <i>The Elements of Statistical Learning</i>.\n",
        "  </em>\n",
        "</p>\n",
        "\n",
        "- **Curva celeste (entrenamiento)**: el error siempre baja al añadir complejidad.  \n",
        "- **Curva roja (test)**: primero baja (captura la señal) y luego sube (captura el **ruido**).  \n",
        "- El **valle mínimo** de la curva roja indica la complejidad que mejor **equilibra precisión y complejidad**.\n",
        "- Separar la muestra permite **detectar ese punto óptimo** y quedarnos con el modelo que realmente aprende patrones útiles.\n",
        "\n",
        "---\n",
        "\n",
        "### Subajuste vs Sobreajuste  \n",
        "\n",
        "| Concepto | Síntomas típicos | Diagnóstico (train / test) |\n",
        "|----------|------------------|------------------------------------|\n",
        "| **Subajuste**<br>(_underfitting_) | • Predicciones pobres en **todos** los conjuntos.<br>• Error alto y estable al añadir datos.<br>• Gráficos de residuos muestran patrones claros. | `ECM/MSE_train` **alto** /`ECM/MSE_test` **alto** |\n",
        "| **Sobreajuste**<br>(_overfitting_) | • Error **muy bajo** en train, **alto** en test.<br>• Predicciones inestables ante pequeñas perturbaciones de datos.<br>• Parámetros con varianzas enormes. | `ECM/MSE_train` **bajo** /`ECM/MSE_test` **alto**|\n",
        "\n",
        "#### ¿Cómo se ve en la curva de la sección anterior?\n",
        "\n",
        "* **Subajuste**: parte izquierda de la curva → error alto en train **y** test.  \n",
        "  El modelo no logra capturar la forma real de los datos.  \n",
        "* **Punto óptimo**: valle de la curva roja → mínimo error de generalización.  \n",
        "  Balance entre precisión y complejidad.\n",
        "* **Sobreajuste**: parte derecha → el error en train sigue bajando,  \n",
        "  pero el de test sube; el modelo “memoriza” ruido.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SS7N1f8pK_qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separación de la Muestra (Train/Test)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(\n",
        "        datos,\n",
        "        test_size=0.30,\n",
        "        random_state=12345)\n",
        "\n",
        "### Modelo de Regresión Lineal (Train)\n",
        "mod_train = smf.ols(\"sales ~ TV + radio + newspaper\", data=train_df).fit()\n",
        "print(mod_train.summary())\n",
        "\n",
        "## Métricas de Bondad de Ajuste - TRAIN\n",
        "r2_adj_train = mod_train.rsquared_adj\n",
        "ecm_train    = mod_train.mse_resid\n",
        "rmse_train   = rmse(mod_train.fittedvalues, train_df.sales)\n",
        "\n",
        "## Métricas de Bondad de Ajuste - TEST\n",
        "# Observado/Predicho\n",
        "y_test      = test_df[\"sales\"]\n",
        "y_hat_test  = mod.predict(test_df)\n",
        "\n",
        "# R² (no ajustado) en TEST\n",
        "sce = ((y_test - y_hat_test)**2).sum()\n",
        "sct = ((y_test - y_test.mean())**2).sum()\n",
        "r2_test = 1 - sce/sct\n",
        "\n",
        "# R² Ajustado en TEST  (n_test = observaciones de test, p = predictores)\n",
        "sce = ((y_test - y_hat_test)**2).sum()\n",
        "sct = ((y_test - y_test.mean())**2).sum()\n",
        "r2_test = 1 - sce/sct\n",
        "n_test = len(test_df)\n",
        "p = len(mod.params) - 1\n",
        "r2_adj_test = 1 - (1 - r2_test)*(n_test - 1)/(n_test-p-1)\n",
        "\n",
        "# ECM y RMSE en TEST\n",
        "ecm_test  = sce/(n_test)\n",
        "rmse_test = np.sqrt(ecm_test)\n",
        "\n",
        "### Resumen\n",
        "resumen = pd.DataFrame({\n",
        "    \"Conjunto\":      [\"TRAIN\", \"TEST\"],\n",
        "    \"R2 Ajustado\":   [r2_adj_train, r2_adj_test],\n",
        "    \"ECM\":           [ecm_train, ecm_test],\n",
        "    \"RMSE\":          [rmse_train, rmse_test]\n",
        "}).set_index(\"Conjunto\")\n",
        "print(resumen)"
      ],
      "metadata": {
        "id": "EHdakD5Vagm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descomposición Sesgo - Varianza"
      ],
      "metadata": {
        "id": "MYmCOP00KpS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Derivación\n",
        "En términos generales, un modelo se puede distinguir por su parte Sistemática y Aleatoria, tal que   $Y = f(X) + \\varepsilon$,  donde $\\mathbb{E}[\\varepsilon] = 0$ y   $\\operatorname{Var}(\\varepsilon) = \\sigma_{\\varepsilon}^{2}$. Sea el ajuste de regresión $\\hat f(X)$ en un punto de entrada $X = x_{0}$, la idea clave es separar el **error de predicción esperado** en tres componentes:\n",
        "\n",
        "$$\n",
        "\\operatorname{Err}(x_0)\n",
        "\\;=\\;\n",
        "\\mathbb{E}\\!\\bigl[(Y-\\hat f(x_0))^2 \\,\\big|\\, X=x_0 \\bigr]\n",
        "\\;=\\;\n",
        "\\underbrace{\\sigma_\\varepsilon^{2}}_{\\text{Error irreducible}}\n",
        "\\;+\\;\n",
        "\\underbrace{\\bigl(\\mathbb{E}[\\hat f(x_0)]-f(x_0)\\bigr)^{2}}_{\\text{Sesgo}^2}\n",
        "\\;+\\;\n",
        "\\underbrace{\\operatorname{Var}\\!\\bigl(\\hat f(x_0)\\bigr)}_{\\text{Varianza}}.\n",
        "$$\n",
        "\n",
        "* **Error irreducible** $(\\sigma_\\varepsilon^{2})$  \n",
        "  La varianza del término aleatorio $\\varepsilon$; no depende del modelo.\n",
        "* **Sesgo**  \n",
        "  Distancia al cuadrado entre la predicción promedio $\\mathbb{E}[\\hat f(x_0)]$ y lo observado $f(x_0)$.\n",
        "* **Varianza**  \n",
        "  Variabilidad de $\\hat f(x_0)$.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e-i9Bv8HKrvl"
      }
    }
  ]
}